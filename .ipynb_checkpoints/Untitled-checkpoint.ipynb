{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "from wordcloud import ImageColorGenerator\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filelist):\n",
    "    \"\"\"\n",
    "    Creates dataframe of the dataset by UCI - Sentiment Analysis \n",
    "\n",
    "    :param filelist: list of file directory.\n",
    "    \"\"\"\n",
    "    #UCI dataset Dataframe\n",
    "    df_uci = pd.concat([pd.read_csv(item, header=None, sep='\\t') for item in filelist], axis=0)\n",
    "    df_uci.columns = ['reviews', 'sentiment']\n",
    "    \n",
    "    #IMDB dataset DataFrame\n",
    "    reviews_train = []\n",
    "    for line in open('/home/yogesh/fall19/ml660/project/movie_data/full_train.txt', 'r'):\n",
    "        reviews_train.append(line.strip())\n",
    "    \n",
    "    df_imdb_train = pd.DataFrame(reviews_train, columns=['reviews'])\n",
    "    \n",
    "    reviews_test = []\n",
    "    for line in open('/home/yogesh/fall19/ml660/project/movie_data/full_test.txt', 'r'):\n",
    "        reviews_test.append(line.strip())\n",
    "    \n",
    "    df_imdb_test = pd.DataFrame(reviews_train, columns=['reviews'])\n",
    "    \n",
    "    return df_uci, df_imdb_train, df_imdb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetselection(df_uci, df_imdb_train, df_imdb_test):\n",
    "    # use argv method to call configuration from the call\n",
    "    \n",
    "    \n",
    "    df_train_data = pd.concat([pd.DataFrame(df_uci['reviews']), df_imdb_train, df_imdb_test], axis=0)\n",
    "    y = pd.concat([pd.DataFrame(df_uci['sentiment']), pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment']), pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment'])])    \n",
    "    return df_train_data, y\n",
    "\n",
    "# df_train_data, y = datasetselection(df_uci, df_imdb_train, df_imdb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfvectorization(df):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = WordNetLemmatizer()\n",
    "\n",
    "    df_array = df['reviews'].to_numpy()\n",
    "    word_list = []\n",
    "    for i in range(len(df_array)):\n",
    "        tokens_new = word_tokenize(df_array[i])\n",
    "        words = [word for word in tokens_new if word.isalpha()]\n",
    "        words = [stemmer.lemmatize(word) for word in words]\n",
    "        doc = ' '.join(words)\n",
    "        word_list.append(doc) \n",
    "    \n",
    "#     return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "\n",
    "#     tfidfconv = TfidfVectorizer(lowercase=True, stop_words=stopwords.words('english'), max_features=2000, min_df=2, max_df=0.7)\n",
    "    tfidfconv = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "    X = tfidfconv.fit_transform(word_list)\n",
    "#     print(X.toarray().shape) \n",
    "    return X\n",
    "\n",
    "# X = tfidfvectorization(df_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(X, y):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    y = y['sentiment'].to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "#     print(X_train.shape)\n",
    "#     print(type(X_test))\n",
    "#     print(y_train.shape)\n",
    "#     print(y_test.shape) \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    parameters = {'C': np.logspace(-2,3,6)}\n",
    "    mod_lr = LogisticRegression()\n",
    "    \n",
    "    clf = GridSearchCV(mod_lr, parameters, cv=5)  # gridsearchCV with 5 fold CV\n",
    "    clf.fit(X_train, y_train)\n",
    "    lambda_scale = 1/clf.best_params_.get('C')  # calculating the best lambda \n",
    "    \n",
    "    \n",
    "    score_scale = clf.best_score_   # Average cross validation score to calculate the error \n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    train_score = 1 - accuracy_score(y_train, y_pred_train)  # Calculating train error \n",
    "    test_score = 1 - accuracy_score(y_test, y_pred_test) # Calculating test error \n",
    "    print(\"-----Logistic Regression--------\") \n",
    "    print(confusion_matrix(y_test,y_pred_test))\n",
    "    print(classification_report(y_test,y_pred_test))\n",
    "    print(accuracy_score(y_test, y_pred_test))\n",
    "    \n",
    "#     sel_ = SelectFromModel(LogisticRegression(C= (1/clf.best_params_.get('C')), penalty='l1'))\n",
    "#     sel_.fit(X_train_fit, y_train)\n",
    "#     print(\"For l:\", i)\n",
    "#     selected_feat = X_train.columns[(sel_.get_support())]\n",
    "#     print('selected features: {}'.format(len(selected_feat)))\n",
    "#     print('c value', clf.best_params_.get('C'))\n",
    "#     print('The best score:', clf.best_score_)\n",
    "#     plist.append(len(selected_feat))\n",
    "#     score.append(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforest(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "    classifier.fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "#     print(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearSVM(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    parameters = {'C': np.logspace(-2,3,6)}\n",
    "    # GridsearchCV used to identify the best parameters from the range declared above. \n",
    "    gs = GridSearchCV(LinearSVC(), parameters, cv = 5)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    #Instanciating of the classifier LinearSVC to train SVC with kernel Linear and l1-penalized with loss = squared hinge loss. \n",
    "    svc = LinearSVC(penalty='l2', loss='squared_hinge', dual = False, C = gs.best_params_.get('C'))\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "#     loss_f_l1 = hamming_loss(y_test.loc[:,'Family'], y_pred_f_l1)\n",
    "\n",
    "    print(\"CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL\")\n",
    "#     print(\"LABEL - FAMILY\")\n",
    "#     print(\"\")\n",
    "    print(\"Test Score is :\", svc.score(X_test, y_test, sample_weight=None))\n",
    "#     print(\"Hamming loss is :\", loss_f_l1)\n",
    "    print(\"Best Penalty is:\", gs.best_params_.get('C'))\n",
    "    print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbfSVM(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    parameters = {'C': np.logspace(-2,3,6), 'gamma': np.linspace(0.05,2)}\n",
    "    #parameters = {'C': np.linspace(0.1, 100, 100), 'gamma': np.linspace(0.05,0.1,2)}\n",
    "\n",
    "    # GridsearchCV used to identify the best parameters from the range declared above. \n",
    "    gs = GridSearchCV(SVC(kernel='rbf'), parameters, cv = 10)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    #Instanciating of the classifier OneVsRest to train SVC with kernel RBF. \n",
    "    estimator = SVC(C = gs.best_params_.get('C'), kernel = 'rbf', gamma = gs.best_params_.get('gamma'))\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred = estimator.predict(X_test)     # Calculates the y_pred value. \n",
    "#     loss_g = hamming_loss(y_test, y_pred)     #The indiviadual hamming loss is calculated. \n",
    "\n",
    "    print(\"CLASSIFICATION USING SVM WITH RBF KERNEL\")\n",
    "#     print(\"LABEL - GENUS\")\n",
    "#     print(\"\")\n",
    "    print(\"Test Score is :\", estimator.score(X_test, y_test, sample_weight=None))\n",
    "#     print(\"Hamming loss is :\", loss_g)\n",
    "    print(\"Best Penalty is:\", gs.best_params_.get('C'), \"and Best Gamma is :\", gs.best_params_.get('gamma'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Logistic Regression--------\n",
      "[[5143  184]\n",
      " [ 180 5043]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      5327\n",
      "           1       0.96      0.97      0.97      5223\n",
      "\n",
      "    accuracy                           0.97     10550\n",
      "   macro avg       0.97      0.97      0.97     10550\n",
      "weighted avg       0.97      0.97      0.97     10550\n",
      "\n",
      "0.9654976303317535\n"
     ]
    }
   ],
   "source": [
    "filelist = ['/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/amazon_cells_labelled.txt', '/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/imdb_labelled.txt', '/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/yelp_labelled.txt']\n",
    "df_uci, df_imdb_train, df_imdb_test = read_file(filelist)\n",
    "\n",
    "df_train_data, y = datasetselection(df_uci, df_imdb_train, df_imdb_test)\n",
    "X = tfidfvectorization(df_train_data)\n",
    "X_train, X_test, y_train, y_test = test_train_split(X, y)\n",
    "# randomforest(X_train, X_test, y_train, y_test) \n",
    "logisticRegression(X_train, X_test, y_train, y_test)\n",
    "# linearSVM(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL\n",
      "Test Score is : 0.9611374407582939\n",
      "Best Penalty is: 10.0\n",
      "0.9611374407582939\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = test_train_split(X, y)\n",
    "# randomforest(X_train, X_test, y_train, y_test) \n",
    "# logisticRegression(X_train, X_test, y_train, y_test)\n",
    "linearSVM(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL\n",
      "Test Score is : 0.9611374407582939\n",
      "Best Penalty is: 10.0\n",
      "0.9611374407582939\n"
     ]
    }
   ],
   "source": [
    "linearSVM(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbfSVM(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
