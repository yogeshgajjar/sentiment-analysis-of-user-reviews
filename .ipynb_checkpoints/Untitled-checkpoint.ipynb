{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "from wordcloud import ImageColorGenerator\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filelist):\n",
    "    \"\"\"\n",
    "    Creates dataframe of the dataset by UCI - Sentiment Analysis \n",
    "\n",
    "    :param filelist: list of file directory.\n",
    "    \"\"\"\n",
    "    #UCI dataset Dataframe\n",
    "    df_uci = pd.concat([pd.read_csv(item, header=None, sep='\\t') for item in filelist], axis=0)\n",
    "    df_uci.columns = ['reviews', 'sentiment']\n",
    "    \n",
    "    #IMDB dataset DataFrame\n",
    "    reviews_train = []\n",
    "    for line in open('/home/yogesh/fall19/ml660/project/movie_data/full_train.txt', 'r'):\n",
    "        reviews_train.append(line.strip())\n",
    "    \n",
    "    df_imdb_train = pd.DataFrame(reviews_train, columns=['reviews'])\n",
    "    \n",
    "    reviews_test = []\n",
    "    for line in open('/home/yogesh/fall19/ml660/project/movie_data/full_test.txt', 'r'):\n",
    "        reviews_test.append(line.strip())\n",
    "    \n",
    "    df_imdb_test = pd.DataFrame(reviews_train, columns=['reviews'])\n",
    "    \n",
    "    return df_uci, df_imdb_train, df_imdb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = ['/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/amazon_cells_labelled.txt', '/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/imdb_labelled.txt', '/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/yelp_labelled.txt']\n",
    "df_uci, df_imdb_train, df_imdb_test = read_file(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2748, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_uci.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uci.to_csv('df_uci.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetselection(df_uci, df_imdb_train, df_imdb_test):\n",
    "    # use argv method to call configuration from the call\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if sys.argv[1] == '1':\n",
    "        print(\"This includes training on 80% of whole dataset and testing on 20%\")\n",
    "        df_train_data = pd.concat([pd.DataFrame(df_uci['reviews']), df_imdb_train, df_imdb_test], axis=0)\n",
    "        y = pd.concat([pd.DataFrame(df_uci['sentiment']), pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment']), pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment'])])    \n",
    "        X = tfidfvectorization(df_train_data) \n",
    "        y = y['sentiment'].to_numpy()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    if sys.argv[2] == '2':\n",
    "        print(\"This includes Train/Test on UCI dataset\")\n",
    "        df_train_data = pd.DataFrame(df_uci['reviews'])\n",
    "        y = pd.DataFrame(df_uci['sentiment'])\n",
    "        X = tfidfvectorization(df_train_data) \n",
    "        y = y['sentiment'].to_numpy()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "        \n",
    "    if sys.argv[3] == '3':\n",
    "        print(\"This includes train on IMDB and test on UCI dataset\")\n",
    "        df_train_data, df_test_data = pd.concat([df_imdb_train, df_imdb_test]), pd.DataFrame(df_uci['reviews'], axis=0)\n",
    "        y_train, y_test = pd.concat([pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment']), pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment'])]), df_uci['sentiment']\n",
    "        X_train, X_test = tfidfvectorization(df_train_data), tfidfvectorization(df_test_data)\n",
    "#         X_train, X_test = shuffle(X.iloc[:50000], random_state=7), shuffle(X.iloc[50000:], random_state=7)\n",
    "#         y_train, y_test = shuffle(y.iloc[:50000], random_state=7), shuffle(y.iloc[50000:], random_state=7)\n",
    "        \n",
    "#         y = y['sentiment'].to_numpy()\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "        \n",
    "    return df_train_data, y\n",
    "\n",
    "# df_train_data, y = datasetselection(df_uci, df_imdb_train, df_imdb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This includes Train/Test on UCI dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"This includes Train/Test on UCI dataset\")\n",
    "df_train_data = pd.DataFrame(df_uci['reviews'])\n",
    "y = pd.DataFrame(df_uci['sentiment'])\n",
    "X = tfidfvectorization(df_train_data) \n",
    "y = y['sentiment'].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This includes train on IMDB and test on UCI dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"This includes train on IMDB and test on UCI dataset\")\n",
    "df_train_data, df_test_data = pd.concat([df_imdb_train, df_imdb_test]), pd.DataFrame(df_uci['reviews'])\n",
    "y_train, y_test = pd.concat([pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment']), pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment'])]), df_uci['sentiment']\n",
    "X_train, X_test = tfidfvectorization(df_train_data), tfidfvectorization(df_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2748x4498 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17493 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfvectorization(df):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = WordNetLemmatizer()\n",
    "\n",
    "    df_array = df['reviews'].to_numpy()\n",
    "    word_list = []\n",
    "    for i in range(len(df_array)):\n",
    "        tokens_new = word_tokenize(df_array[i])\n",
    "        words = [word for word in tokens_new if word.isalpha()]\n",
    "        words = [stemmer.lemmatize(word) for word in words]\n",
    "        doc = ' '.join(words)\n",
    "        word_list.append(doc) \n",
    "    \n",
    "#     return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "\n",
    "#     tfidfconv = TfidfVectorizer(lowercase=True, stop_words=stopwords.words('english'), max_features=2000, min_df=2, max_df=0.7)\n",
    "    tfidfconv = TfidfVectorizer(max_features=4000, stop_words=stopwords.words('english'))\n",
    "    X = tfidfconv.fit_transform(word_list)\n",
    "#     print(X.toarray().shape) \n",
    "    return X\n",
    "\n",
    "# X = tfidfvectorization(df_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(X, y):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    y = y['sentiment'].to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "#     print(X_train.shape)\n",
    "#     print(type(X_test))\n",
    "#     print(y_train.shape)\n",
    "#     print(y_test.shape) \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestB(X,y):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    X = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    parameters = {'C': np.logspace(-2,3,6)}\n",
    "    mod_lr = LogisticRegression()\n",
    "    \n",
    "    clf = GridSearchCV(mod_lr, parameters, cv=5)  # gridsearchCV with 5 fold CV\n",
    "    clf.fit(X_train, y_train)\n",
    "    lambda_scale = 1/clf.best_params_.get('C')  # calculating the best lambda \n",
    "    \n",
    "    \n",
    "    score_scale = clf.best_score_   # Average cross validation score to calculate the error \n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    train_score = 1 - accuracy_score(y_train, y_pred_train)  # Calculating train error \n",
    "    test_score = 1 - accuracy_score(y_test, y_pred_test) # Calculating test error \n",
    "    print(\"-----Logistic Regression--------\") \n",
    "    print(confusion_matrix(y_test,y_pred_test))\n",
    "    print(classification_report(y_test,y_pred_test))\n",
    "    print(accuracy_score(y_test, y_pred_test))\n",
    "    \n",
    "#     sel_ = SelectFromModel(LogisticRegression(C= (1/clf.best_params_.get('C')), penalty='l1'))\n",
    "#     sel_.fit(X_train_fit, y_train)\n",
    "#     print(\"For l:\", i)\n",
    "#     selected_feat = X_train.columns[(sel_.get_support())]\n",
    "#     print('selected features: {}'.format(len(selected_feat)))\n",
    "#     print('c value', clf.best_params_.get('C'))\n",
    "#     print('The best score:', clf.best_score_)\n",
    "#     plist.append(len(selected_feat))\n",
    "#     score.append(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforest(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "    classifier.fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "#     print(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearSVM(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    parameters = {'C': np.logspace(-2,3,6)}\n",
    "    # GridsearchCV used to identify the best parameters from the range declared above. \n",
    "    gs = GridSearchCV(LinearSVC(), parameters, cv = 5)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    #Instanciating of the classifier LinearSVC to train SVC with kernel Linear and l1-penalized with loss = squared hinge loss. \n",
    "    svc = LinearSVC(penalty='l2', loss='squared_hinge', dual = False, C = gs.best_params_.get('C'))\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "#     loss_f_l1 = hamming_loss(y_test.loc[:,'Family'], y_pred_f_l1)\n",
    "\n",
    "    print(\"CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL\")\n",
    "#     print(\"LABEL - FAMILY\")\n",
    "#     print(\"\")\n",
    "    print(\"Test Score is :\", svc.score(X_test, y_test, sample_weight=None))\n",
    "#     print(\"Hamming loss is :\", loss_f_l1)\n",
    "    print(\"Best Penalty is:\", gs.best_params_.get('C'))\n",
    "    print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbfSVM(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    parameters = {'C': np.logspace(-2,3,6), 'gamma': np.linspace(0.05,2)}\n",
    "    #parameters = {'C': np.linspace(0.1, 100, 100), 'gamma': np.linspace(0.05,0.1,2)}\n",
    "\n",
    "    # GridsearchCV used to identify the best parameters from the range declared above. \n",
    "    gs = GridSearchCV(SVC(kernel='rbf'), parameters, cv = 10)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    #Instanciating of the classifier OneVsRest to train SVC with kernel RBF. \n",
    "    estimator = SVC(C = gs.best_params_.get('C'), kernel = 'rbf', gamma = gs.best_params_.get('gamma'))\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred = estimator.predict(X_test)     # Calculates the y_pred value. \n",
    "#     loss_g = hamming_loss(y_test, y_pred)     #The indiviadual hamming loss is calculated. \n",
    "\n",
    "    print(\"CLASSIFICATION USING SVM WITH RBF KERNEL\")\n",
    "#     print(\"LABEL - GENUS\")\n",
    "#     print(\"\")\n",
    "    print(\"Test Score is :\", estimator.score(X_test, y_test, sample_weight=None))\n",
    "#     print(\"Hamming loss is :\", loss_g)\n",
    "    print(\"Best Penalty is:\", gs.best_params_.get('C'), \"and Best Gamma is :\", gs.best_params_.get('gamma'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Logistic Regression--------\n",
      "[[5143  184]\n",
      " [ 180 5043]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      5327\n",
      "           1       0.96      0.97      0.97      5223\n",
      "\n",
      "    accuracy                           0.97     10550\n",
      "   macro avg       0.97      0.97      0.97     10550\n",
      "weighted avg       0.97      0.97      0.97     10550\n",
      "\n",
      "0.9654976303317535\n"
     ]
    }
   ],
   "source": [
    "filelist = ['/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/amazon_cells_labelled.txt', '/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/imdb_labelled.txt', '/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/yelp_labelled.txt']\n",
    "df_uci, df_imdb_train, df_imdb_test = read_file(filelist)\n",
    "\n",
    "df_train_data, y = datasetselection(df_uci, df_imdb_train, df_imdb_test)\n",
    "X = tfidfvectorization(df_train_data)\n",
    "X_train, X_test, y_train, y_test = test_train_split(X, y)\n",
    "# randomforest(X_train, X_test, y_train, y_test) \n",
    "logisticRegression(X_train, X_test, y_train, y_test)\n",
    "# linearSVM(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL\n",
      "Test Score is : 0.9611374407582939\n",
      "Best Penalty is: 10.0\n",
      "0.9611374407582939\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = test_train_split(X, y)\n",
    "# randomforest(X_train, X_test, y_train, y_test) \n",
    "# logisticRegression(X_train, X_test, y_train, y_test)\n",
    "linearSVM(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL\n",
      "Test Score is : 0.9611374407582939\n",
      "Best Penalty is: 10.0\n",
      "0.9611374407582939\n"
     ]
    }
   ],
   "source": [
    "linearSVM(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbfSVM(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This includes train on IMDB and test on UCI dataset\n",
      "(50000, 4000)\n",
      "(2748, 4000)\n",
      "(2748,)\n",
      "(50000, 1)\n",
      "-----Logistic Regression--------\n",
      "[[694 668]\n",
      " [722 664]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.51      0.50      1362\n",
      "           1       0.50      0.48      0.49      1386\n",
      "\n",
      "    accuracy                           0.49      2748\n",
      "   macro avg       0.49      0.49      0.49      2748\n",
      "weighted avg       0.49      0.49      0.49      2748\n",
      "\n",
      "0.49417758369723436\n"
     ]
    }
   ],
   "source": [
    "print(\"This includes train on IMDB and test on UCI dataset\")\n",
    "df_train_data, df_test_data = pd.concat([df_imdb_train, df_imdb_test]), pd.DataFrame(df_uci['reviews'])\n",
    "y_train, y_test = pd.concat([pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment']), pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment'])]), df_uci['sentiment']\n",
    "X_train, X_test = tfidfvectorization(df_train_data), tfidfvectorization(df_test_data)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "parameters = {'C': np.logspace(-2,3,6)}\n",
    "mod_lr = LogisticRegression()\n",
    "\n",
    "clf = GridSearchCV(mod_lr, parameters, cv=5)  # gridsearchCV with 5 fold CV\n",
    "clf.fit(X_train, y_train)\n",
    "lambda_scale = 1/clf.best_params_.get('C')  # calculating the best lambda \n",
    "\n",
    "\n",
    "score_scale = clf.best_score_   # Average cross validation score to calculate the error \n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "train_score = 1 - accuracy_score(y_train, y_pred_train)  # Calculating train error \n",
    "test_score = 1 - accuracy_score(y_test, y_pred_test) # Calculating test error \n",
    "print(\"-----Logistic Regression--------\") \n",
    "print(confusion_matrix(y_test,y_pred_test))\n",
    "print(classification_report(y_test,y_pred_test))\n",
    "print(accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCI_train_data = []\n",
    "UCI_train_labels = []\n",
    "\n",
    "with open(\"/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/amazon_cells_labelled.txt\", 'r') as f:\n",
    "    content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "\n",
    "for review in content:\n",
    "    UCI_train_data.append(review.split(\"\\t\")[0])\n",
    "    UCI_train_labels.append(review.split(\"\\t\")[1])\n",
    "\n",
    "with open(\"/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/imdb_labelled.txt\", 'r') as f:\n",
    "    content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "\n",
    "for review in content:\n",
    "    UCI_train_data.append(review.split(\"\\t\")[0])\n",
    "    UCI_train_labels.append(review.split(\"\\t\")[1])\n",
    "\n",
    "with open(\"/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/yelp_labelled.txt\", 'r') as f:\n",
    "    content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "\n",
    "for review in content:\n",
    "    UCI_train_data.append(review.split(\"\\t\")[0])\n",
    "    UCI_train_labels.append(review.split(\"\\t\")[1])\n",
    "\n",
    "df_uci_train = pd.DataFrame(UCI_train_data, columns=['reviews'])\n",
    "df_uci_labels = pd.DataFrame(UCI_train_labels, columns=['sentiment'])\n",
    "df_uci = pd.concat([df_uci_train, df_uci_labels], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_uci.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "from wordcloud import ImageColorGenerator\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uci_data():\n",
    "    UCI_train_data = []\n",
    "    UCI_train_labels = []\n",
    "\n",
    "    with open(\"/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/amazon_cells_labelled.txt\", 'r') as f:\n",
    "        content = f.readlines()\n",
    "        content = [x.strip() for x in content] \n",
    "\n",
    "    for review in content:\n",
    "        UCI_train_data.append(review.split(\"\\t\")[0])\n",
    "        UCI_train_labels.append(review.split(\"\\t\")[1])\n",
    "\n",
    "    with open(\"/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/imdb_labelled.txt\", 'r') as f:\n",
    "        content = f.readlines()\n",
    "        content = [x.strip() for x in content] \n",
    "\n",
    "    for review in content:\n",
    "        UCI_train_data.append(review.split(\"\\t\")[0])\n",
    "        UCI_train_labels.append(review.split(\"\\t\")[1])\n",
    "\n",
    "    with open(\"/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/yelp_labelled.txt\", 'r') as f:\n",
    "        content = f.readlines()\n",
    "        content = [x.strip() for x in content] \n",
    "\n",
    "    for review in content:\n",
    "        UCI_train_data.append(review.split(\"\\t\")[0])\n",
    "        UCI_train_labels.append(review.split(\"\\t\")[1])\n",
    "        \n",
    "    df_uci_train = pd.DataFrame(UCI_train_data, columns=['reviews'])\n",
    "    df_uci_labels = pd.DataFrame(UCI_train_labels, columns=['sentiment'])\n",
    "    df_uci = pd.concat([df_uci_train, df_uci_labels], axis = 1)\n",
    "    return df_uci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filelist):\n",
    "    \"\"\"\n",
    "    Creates dataframe of the dataset by UCI - Sentiment Analysis \n",
    "\n",
    "    :param filelist: list of file directory.\n",
    "    \"\"\"\n",
    "    #UCI dataset Dataframe\n",
    "#     df_uci = pd.concat([pd.read_csv(item, header=None, sep='\\t') for item in filelist], axis=0)\n",
    "    df_uci.columns = ['reviews', 'sentiment']\n",
    "#     df_uci = uci_data()\n",
    "    #IMDB dataset DataFrame\n",
    "    reviews_train = []\n",
    "    for line in open('/home/yogesh/fall19/ml660/project/movie_data/full_train.txt', 'r'):\n",
    "        reviews_train.append(line.strip())\n",
    "    \n",
    "    df_imdb_train = pd.DataFrame(reviews_train, columns=['reviews'])\n",
    "    \n",
    "    reviews_test = []\n",
    "    for line in open('/home/yogesh/fall19/ml660/project/movie_data/full_test.txt', 'r'):\n",
    "        reviews_test.append(line.strip())\n",
    "    \n",
    "    df_imdb_test = pd.DataFrame(reviews_train, columns=['reviews'])\n",
    "    \n",
    "    return df_uci, df_imdb_train, df_imdb_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train(df_uci, df_imdb_test, df_imdb_train, target):\n",
    "    if (target == 1): #2199\n",
    "        df_train_data = shuffle(pd.concat([pd.DataFrame(df_uci.iloc[0:2401,0]), df_imdb_train, df_imdb_test], axis=0), random_state = 7)\n",
    "        df_test_data = shuffle(pd.DataFrame(df_uci.iloc[2401:,0]), random_state=7)\n",
    "        y_train = shuffle(pd.concat([pd.DataFrame(df_uci.iloc[0:2401, 1]), pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment']), pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment'])]), random_state = 7)   \n",
    "        y_test = shuffle(pd.DataFrame(df_uci.loc[2401:, 1]), random_state = 7)\n",
    "\n",
    "    if (target == 2):\n",
    "        df_train_data = pd.concat([pd.DataFrame(df_uci['reviews']), df_imdb_train, df_imdb_test.iloc[0:20000]], axis=0)\n",
    "        df_test_data = df_imdb_test.iloc[20000:]\n",
    "        y_train = pd.concat([pd.DataFrame(df_uci['sentiment']), pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment']), pd.DataFrame([1 if i < 12500 else 0 for i in range(20000)], columns=['sentiment'])])    \n",
    "        y_test = pd.DataFrame([1 for i in range(5000)], columns=['sentiment'])\n",
    "    \n",
    "    return df_train_data, df_test_data, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetselection(df_uci, df_imdb_train, df_imdb_test):\n",
    "    # use argv method to call configuration from the call\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "#     if sys.argv[1] == '1':\n",
    "    print(\"This includes training on 80% of whole dataset and testing on 20%\")\n",
    "    df_train_data = pd.concat([pd.DataFrame(df_uci['reviews']), df_imdb_train, df_imdb_test], axis=0)\n",
    "    y = pd.concat([pd.DataFrame(df_uci['sentiment']), pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment']), pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment'])])    \n",
    "    X = tfidfvectorization(df_train_data) \n",
    "    y = y['sentiment'].to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    \n",
    "#     elif sys.argv[1] == '2':\n",
    "#     print(\"This includes Train/Test on UCI dataset\")\n",
    "#     df_train_data = pd.DataFrame(df_uci['reviews'])\n",
    "#     y = pd.DataFrame(df_uci['sentiment'])\n",
    "#     X = tfidfvectorization(df_train_data) \n",
    "#     y = y['sentiment'].to_numpy()\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#     elif sys.argv[1] == '3':\n",
    "#         print(\"This includes train on IMDB and test on UCI dataset\")\n",
    "#         df_train_data, df_test_data = shuffle(pd.concat([df_imdb_train, df_imdb_test]), random_state = 7), shuffle(pd.DataFrame(df_uci['reviews']), random_state = 7)\n",
    "#         y_train, y_test = shuffle(pd.concat([pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment']), pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment'])]), random_state = 7), shuffle(df_uci['sentiment'], random_state = 7)\n",
    "#         X_train, X_test = tfidfvectorization(df_train_data), tfidfvectorization(df_test_data)\n",
    "\n",
    "#     elif sys.argv[1] == '4':\n",
    "#         print(\"This includes Train/Test on IMDB dataset\")\n",
    "#         y_train, y_test = shuffle(pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment']), random_state = 7), shuffle(pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment']), random_state = 7)\n",
    "#         df_imdb_train, df_imdb_test = shuffle(df_imdb_train, random_state = 7), shuffle(df_imdb_test, random_state = 7)\n",
    "#         X_train, X_test = tfidfvectorization(df_imdb_train), tfidfvectorization(df_imdb_test)\n",
    "        \n",
    "#     elif sys.argv[1] == '5':\n",
    "#         print(\"This includes Training on 100% IMDB + 80% UCI and Test on 20% UCI\")\n",
    "#         df_train_data, df_test_data, y_train, y_test = split_train(df_uci, df_imdb_test, df_imdb_train, 1)\n",
    "#         X_train, X_test = tfidfvectorization(df_imdb_train), tfidfvectorization(df_test_data)\n",
    "\n",
    "\n",
    "#     elif sys.argv[1] == '6':\n",
    "#         print(\"This includes Training on 100% UCI + 80% IMDB and Test on 20% IMDB\")\n",
    "\n",
    "#     else:\n",
    "#         print(\"done\")\n",
    "    # print(\"This includes training on 80% of whole dataset and testing on 20%\")\n",
    "    # df_train_data = pd.concat([pd.DataFrame(df_uci['reviews']), df_imdb_train, df_imdb_test], axis=0)\n",
    "    # y = pd.concat([pd.DataFrame(df_uci['sentiment']), pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment']), pd.DataFrame([1 if i < 12500 else 0 for i in range(25000)], columns=['sentiment'])])    \n",
    "    \n",
    "    # if sys.argv[2] == '2':\n",
    "    #     print(\"This includes training on 100% UCI dataset + 80% of Imdb dataset and testing on 20% Imdb dataset\")\n",
    "    #     df_train_data = pd.concat([pd.DataFrame(df_uci['reviews']), ])\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    print(y_train.shape) \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = ['/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/amazon_cells_labelled.txt', '/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/imdb_labelled.txt', '/home/yogesh/fall19/ml660/project/sentiment_labelled_sentences/yelp_labelled.txt']\n",
    "df_uci, df_imdb_train, df_imdb_test = read_file(filelist)\n",
    "\n",
    "X_train, X_test, y_train, y_test = datasetselection(df_uci, df_imdb_train, df_imdb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfvectorization(df):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = WordNetLemmatizer()\n",
    "\n",
    "    df_array = df['reviews'].to_numpy()\n",
    "    word_list = []\n",
    "    for i in range(len(df_array)):\n",
    "        tokens_new = word_tokenize(df_array[i])\n",
    "        words = [word for word in tokens_new if word.isalpha()]\n",
    "        words = [stemmer.lemmatize(word) for word in words]\n",
    "        doc = ' '.join(words)\n",
    "        word_list.append(doc) \n",
    "    \n",
    "#     return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "\n",
    "#     tfidfconv = TfidfVectorizer(lowercase=True, stop_words=stopwords.words('english'), max_features=2000, min_df=2, max_df=0.7)\n",
    "    # if sys.argv[1] == '3':\n",
    "    #     tfidfconv = TfidfVectorizer(stop_words=stopwords.words('english'), max_features=4000, min_df=2, max_df=0.7)\n",
    "    #     X = tfidfconv.fit_transform(word_list)\n",
    "\n",
    "    tfidfconv = TfidfVectorizer(lowercase=True, stop_words=stopwords.words('english'))\n",
    "    X = tfidfconv.fit_transform(word_list)\n",
    "\n",
    "#     print(X.toarray().shape) \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(X, y):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    y = y['sentiment'].to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "#     print(X_train.shape)\n",
    "#     print(type(X_test))\n",
    "#     print(y_train.shape)\n",
    "#     print(y_test.shape) \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
